{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33f9e60",
   "metadata": {
    "id": "f33f9e60"
   },
   "source": [
    "This can be [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework-3/blob/main/HW3.3-Visualization_Examples.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "pip3 install git+https://github.com/davidbau/baukit@main#egg=baukit"
   ],
   "metadata": {
    "id": "2wcWqG314Tnq"
   },
   "id": "2wcWqG314Tnq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2708f",
   "metadata": {
    "id": "cbd2708f"
   },
   "outputs": [],
   "source": [
    "import torch, os, PIL.Image, numpy\n",
    "from torchvision.models import alexnet, resnet18, resnet101, resnet152, efficientnet_b1\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from baukit import ImageFolderSet, show, renormalize, set_requires_grad, Trace, pbar\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7929a1d",
   "metadata": {
    "id": "c7929a1d"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget -N https://cs7150.baulab.info/2022-Fall/data/dog-and-cat-example.jpg\n",
    "wget -N https://cs7150.baulab.info/2022-Fall/data/hungry-cat.jpg\n",
    "wget -N https://cs7150.baulab.info/2022-Fall/data/imagenet-labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7fada",
   "metadata": {
    "id": "fdf7fada"
   },
   "source": [
    "## Visualizing the behavior of a convolutional network\n",
    "\n",
    "Here we brieflly overview some of the major categories of methods for visualizing the behavior of a convolutional network classifier: occlusion, gradients, class activation maps (CAM), and dissection.\n",
    "\n",
    "Let's define some utility functions for manipulating images. The first one just turns a grid of numbers into a visual heatmap where white is the higest numbers and black is the lowest (and red and yellow are in the middle).\n",
    "\n",
    "Another is for making a theshold mask instead of a heatmap, to just highlight the highest regions.\n",
    "\n",
    "And then another one creates an overlay between two images.\n",
    "\n",
    "With these in hand, we can create some salience map visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed336ad2",
   "metadata": {
    "id": "ed336ad2"
   },
   "outputs": [],
   "source": [
    "def rgb_heatmap(data, size=None, colormap='hot', amax=None, amin=None, mode='bicubic', symmetric=False):\n",
    "    size = spec_size(size)\n",
    "    mapping = getattr(cm, colormap)\n",
    "    scaled = torch.nn.functional.interpolate(data[None, None], size=size, mode=mode)[0,0]\n",
    "    if amax is None: amax = data.max()\n",
    "    if amin is None: amin = data.min()\n",
    "    if symmetric:\n",
    "        amax = max(amax, -amin)\n",
    "        amin = min(amin, -amax)\n",
    "    normed = ((scaled - amin) / (amax - amin + 1e-10)).numpy()\n",
    "    return PIL.Image.fromarray((255 * mapping(normed)).astype('uint8'))\n",
    "\n",
    "def rgb_threshold(data, size=None, mode='bicubic', p=0.2):\n",
    "    size = spec_size(size)\n",
    "    scaled = torch.nn.functional.interpolate(data[None, None], size=size, mode=mode)[0,0]\n",
    "    ordered = scaled.view(-1).sort()[0]\n",
    "    threshold = ordered[int(len(ordered) * (1-p))]\n",
    "    result = numpy.tile((scaled > threshold)[:,:,None], (1, 1, 3))\n",
    "    return PIL.Image.fromarray((255 * result).astype('uint8'))\n",
    "\n",
    "def overlay(im1, im2, alpha=0.5):\n",
    "    import numpy\n",
    "    return PIL.Image.fromarray((\n",
    "        numpy.array(im1)[...,:3] * alpha +\n",
    "        numpy.array(im2)[...,:3] * (1 - alpha)).astype('uint8'))\n",
    "\n",
    "def overlay_threshold(im1, im2, alpha=0.5):\n",
    "    import numpy\n",
    "    return PIL.Image.fromarray((\n",
    "        numpy.array(im1)[...,:3] * (1 - numpy.array(im2)[...,:3]/255) * alpha +\n",
    "        numpy.array(im2)[...,:3] * (numpy.array(im1)[...,:3]/255)).astype('uint8'))\n",
    "\n",
    "def spec_size(size):\n",
    "    if isinstance(size, int): dims = (size, size)\n",
    "    if isinstance(size, torch.Tensor): size = size.shape[:2]\n",
    "    if isinstance(size, PIL.Image.Image): size = (size.size[1], size.size[0])\n",
    "    if size is None: size = (224, 224)\n",
    "    return size\n",
    "\n",
    "def resize_and_crop(im, d):\n",
    "    if im.size[0] >= im.size[1]:\n",
    "        im = im.resize((int(im.size[0]/im.size[1]*d), d))\n",
    "        return im.crop(((im.size[0] - d) // 2, 0, (im.size[0] + d) // 2, d))\n",
    "    else:\n",
    "        im = im.resize((d, int(im.size[1]/im.size[9]*d)))\n",
    "        return im.crop((0, (im.size[1] - d) // 2, d, (im.size[1] + d) // 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5639d",
   "metadata": {
    "id": "b5d5639d"
   },
   "source": [
    "## Loading a pretrained classifier and an example image\n",
    "\n",
    "Here is an example image, and an example network.\n",
    "\n",
    "We will look at a resnet18.  You could do any network, e.g. try a resnet152..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1875e8",
   "metadata": {
    "id": "7e1875e8"
   },
   "outputs": [],
   "source": [
    "im = resize_and_crop(PIL.Image.open('dog-and-cat-example.jpg'), 224)\n",
    "show(im)\n",
    "data = renormalize.from_image(resize_and_crop(im, 224), target='imagenet')\n",
    "with open('imagenet-labels.txt') as r:\n",
    "    labels = [line.split(',')[1].strip() for line in r.readlines()]\n",
    "net = resnet18(pretrained=True)\n",
    "net.eval()\n",
    "set_requires_grad(False, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ccb28",
   "metadata": {
    "id": "581ccb28"
   },
   "source": [
    "## Visualization using occlusion\n",
    "\n",
    "First, let's try a method suggested by Zeiler 2014.  Slide a window across the image and test each version.\n",
    "\n",
    "https://arxiv.org/pdf/1311.2901.pdf\n",
    "\n",
    "The following is a function for creating a series of sliding-window masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14056d5",
   "metadata": {
    "id": "c14056d5"
   },
   "outputs": [],
   "source": [
    "def sliding_window(dims=None, window=1, stride=1, hole=True):\n",
    "    dims = spec_size(dims)\n",
    "    assert(len(dims) == 2)\n",
    "    for y in range(0, dims[0], stride):\n",
    "        for x in range(0, dims[1], stride):\n",
    "            mask = torch.zeros(*dims)\n",
    "            mask[y:y+window, x:x+window] = 1\n",
    "            if hole:\n",
    "                mask = 1 - mask\n",
    "            yield mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75270c75",
   "metadata": {
    "id": "75270c75"
   },
   "source": [
    "We will create a batch of masks,   and then we will create a `masked_batch` batch of images which have a gray square masked in in each of them.  We will create some 196 versions of this masked image.\n",
    "\n",
    "Below is an example picture of one of the masked images, where the mask happens to cover the dog's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99009bef",
   "metadata": {
    "id": "99009bef"
   },
   "outputs": [],
   "source": [
    "masks = torch.stack(list(sliding_window(im, window=48, stride=16)))\n",
    "masks = masks[:, None, :, :]\n",
    "print('masks', masks.shape)\n",
    "\n",
    "masked_batch = data * masks\n",
    "print('masked_batch', masked_batch.shape)\n",
    "\n",
    "show(renormalize.as_image(masked_batch[19]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f18ee4",
   "metadata": {
    "id": "21f18ee4"
   },
   "source": [
    "Now let's run the network to get its predictions.\n",
    "\n",
    "But also we will run the network on each of the masked images.\n",
    "\n",
    "Notice that this image is guessed as both a dog ('boxer') and cat ('tiger cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dffea2",
   "metadata": {
    "scrolled": true,
    "id": "99dffea2"
   },
   "outputs": [],
   "source": [
    "base_preds = net(data[None])\n",
    "masked_preds = net(masked_batch)\n",
    "[(labels[i], i.item()) for i in base_preds.topk(dim=1, k=5, sorted=True)[1][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78104b",
   "metadata": {
    "id": "8d78104b"
   },
   "source": [
    "<b>Exercise 3.3.1:</b> What are the predictions of the network for the masked image shown above? Print them out like we did above. What do you think happened here? Give your thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7f594",
   "metadata": {
    "id": "6ec7f594"
   },
   "outputs": [],
   "source": [
    "# Type your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02514e",
   "metadata": {
    "id": "4a02514e"
   },
   "source": [
    "<b>Exercise 3.3.2:</b> For each of the masked image, we have predictions. \n",
    "- Show the image that has least score for boxer\n",
    "- Show the image that has least score for tiger cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c31e5d",
   "metadata": {
    "id": "71c31e5d"
   },
   "outputs": [],
   "source": [
    "# Type your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daada3b4",
   "metadata": {
    "id": "daada3b4"
   },
   "source": [
    "Here is a way that we can visualise the pixels that are more responsible for the predictions. It's something similar you did above in Exercise 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972422a",
   "metadata": {
    "id": "0972422a"
   },
   "outputs": [],
   "source": [
    "for c in ['boxer', 'tiger cat']:\n",
    "    heatmap = (base_preds[:,labels.index(c)]-masked_preds[:,labels.index(c)]).view(14,14)\n",
    "    show(show.TIGHT, [[\n",
    "        [c, rgb_heatmap(heatmap, mode='nearest', symmetric=True)],\n",
    "        ['ovarlay', overlay(im, rgb_heatmap(heatmap, symmetric=True))]\n",
    "        \n",
    "    ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860bddd",
   "metadata": {
    "id": "7860bddd"
   },
   "source": [
    "## Visualization using smoothgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e5e92",
   "metadata": {
    "id": "683e5e92"
   },
   "source": [
    "Since neural networks are differentiable, it is natural to try to visualize them using gradients.\n",
    "\n",
    "One simple method is smoothgrad (Smilkov 2017), which examines gradients of perturbed inputs.\n",
    "\n",
    "https://arxiv.org/pdf/1706.03825.pdf\n",
    "\n",
    "The concept is, \"according to gradients, which pixels most affect the prediction of the given class?\"\n",
    "\n",
    "Although gradients are a neat idea, it can be hard to get them to work well for visualization.  See Adebayo 2018\n",
    "\n",
    "https://arxiv.org/pdf/1810.03292.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732836d0",
   "metadata": {
    "id": "732836d0"
   },
   "source": [
    "<b>Exercise 3.3.3</b>: In this exercise, we will see the gradient wrt to the image. Please replace the value `None` in `gradient=None` with the gradient wrt to input(in this case a smoothened input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d808485",
   "metadata": {
    "id": "4d808485"
   },
   "outputs": [],
   "source": [
    "for label in ['boxer', 'tiger cat']:\n",
    "    total = 0\n",
    "    for i in range(20):\n",
    "        prober = data + torch.randn(data.shape) * 0.2\n",
    "        prober.requires_grad = True\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            net(prober[None]),\n",
    "            torch.tensor([labels.index(label)]))\n",
    "        loss.backward()\n",
    "        \n",
    "        gradient = None # TO-DO (Replace None with the gradient wrt to the perturbed input)\n",
    "        \n",
    "        total += gradient**2\n",
    "        prober.grad = None\n",
    "\n",
    "    show(show.TIGHT, [[\n",
    "        [label,\n",
    "         renormalize.as_image(data, source='imagenet')],\n",
    "        ['total grad**2',\n",
    "         renormalize.as_image((total / total.max() * 5).clamp(0, 1), source='pt')],\n",
    "        ['overlay',\n",
    "         overlay(renormalize.as_image(data, source='imagenet'),\n",
    "                renormalize.as_image((total / total.max() * 5).clamp(0, 1), source='pt'))]\n",
    "    ]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa44c1f",
   "metadata": {
    "id": "ffa44c1f"
   },
   "source": [
    "## Single neuron dissection\n",
    "\n",
    "In this code, we ask \"What does a single kind of neuron detect\", e.g., the neurons of the 100th convolutional filter of the layer4.0.conv1 layer of resnet18.\n",
    "\n",
    "To see that, we use dissection to visualize the neurons (Bau 2017).\n",
    "\n",
    "https://arxiv.org/pdf/1704.05796.pdf\n",
    "\n",
    "We run the network over a large sample of images (here we use 5000 random images from the imagenet validation set), and we show the 12 regions where the neuron activated strongest in this data set.\n",
    "\n",
    "Can you see a pattern for neuron 100?  What about for neuron 200 or neuron 50?\n",
    "\n",
    "Some neurons activate on more than one concept.  Some neurons are more understandable than others.\n",
    "\n",
    "Below, we begin by loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d3b91",
   "metadata": {
    "id": "9c5d3b91"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('imagenet_val_5k'):\n",
    "    download_and_extract_archive('https://cs7150.baulab.info/2022-Fall/data/imagenet_val_5k.zip',\n",
    "                                 'imagenet_val_5k')\n",
    "ds = ImageFolderSet('imagenet_val_5k', shuffle=True, transform=Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    renormalize.NORMALIZER['imagenet']\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097c738",
   "metadata": {
    "id": "e097c738"
   },
   "source": [
    "The following code examines the top-activating neurons in a particular convolutional layer, for our test image.\n",
    "\n",
    "Which is the first neuron that activates for the cat but not the dog?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b8969",
   "metadata": {
    "id": "049b8969"
   },
   "source": [
    "Let's dissect the first filter output of the layer4.1.conv1 and see what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b9f52",
   "metadata": {
    "id": "473b9f52"
   },
   "outputs": [],
   "source": [
    "layer = 'layer4.1.conv1'\n",
    "unit_num = 0 \n",
    "with Trace(net, layer) as tr:\n",
    "    preds = net(data[None])\n",
    "show(show.WRAP, [[f'neuron {unit_num}',\n",
    "        overlay(im, rgb_heatmap(tr.output[0, unit_num]))]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c58d6",
   "metadata": {
    "id": "224c58d6"
   },
   "source": [
    "<b>Exercise 3.3</b>: The above representation is for filter 0. Now visualise the top 12 filters that activate the most. <br>\n",
    "[Hint: To do this, we recommend using max values of each filter and show the top 12 filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc7133",
   "metadata": {
    "id": "11cc7133"
   },
   "outputs": [],
   "source": [
    "# Type your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141d41f",
   "metadata": {
    "id": "f141d41f"
   },
   "source": [
    "<b>Exercise 3.4</b>: Which of the top filters is activating the cat more? \n",
    "\n",
    "\n",
    "Choose one and run the network on all the data and sort to find the maximum-activating data. Let's see how the neuron you found to be top activating generalizes. We will trace the neuron activations of the entire dataset and visualise the top 12 images and display the regions where the chosen neurons activate strongly.\n",
    "\n",
    "Here we select neuron number 0 in layer4.1.conv1 to show how you can do it. Replace it with the number you found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912c8e7",
   "metadata": {
    "id": "f912c8e7"
   },
   "outputs": [],
   "source": [
    "def dissect_unit(ds, i, net, layer, unit):\n",
    "    data = ds[i][0]\n",
    "    with Trace(net, layer) as tr:\n",
    "        net(data[None])\n",
    "    mask = rgb_threshold(tr.output[0, unit], size=data.shape[-2:])\n",
    "    img = renormalize.as_image(data, source=ds)\n",
    "    return overlay_threshold(img, mask)\n",
    "\n",
    "neuron = 0\n",
    "scores = []\n",
    "for imagenum, [d,] in enumerate(pbar(ds)):\n",
    "    with Trace(net, layer) as tr:\n",
    "        _ = net(d[None])\n",
    "    score = tr.output[0, neuron].view(-1).max()\n",
    "    scores.append((score, imagenum))\n",
    "scores.sort(reverse=True)\n",
    "\n",
    "show(f'{layer} neuron {neuron}',\n",
    "     [[dissect_unit(ds, scores[i][1], net, layer, neuron) for i in range(12)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875b5e2b",
   "metadata": {
    "id": "875b5e2b"
   },
   "source": [
    "<b>Exercise 3.5</b>: Is the neuron only activating cats? How well do you think it is generalising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1e7fe",
   "metadata": {
    "id": "71b1e7fe"
   },
   "source": [
    "Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fcdf9",
   "metadata": {
    "id": "2f5fcdf9"
   },
   "source": [
    "## Visualization using grad-cam\n",
    "\n",
    "Another idea is to look at gradients to the interior activations rather than gradients all the way to the pixels.  CAM (Zhou 2015) and Grad-CAM (Selvaraju 2016) do that.\n",
    "\n",
    "https://arxiv.org/pdf/1512.04150.pdf\n",
    "https://arxiv.org/pdf/1610.02391.pdf\n",
    "\n",
    "Grad-cam works by examiming internal network activations; to do that we will use the `Trace` class from baukit.\n",
    "\n",
    "So we run the network again in inference to classify the image, this time tracing the output of the last convolutional layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857d73b",
   "metadata": {
    "id": "0857d73b"
   },
   "outputs": [],
   "source": [
    "with Trace(net, 'layer4') as tr:\n",
    "    preds = net(data[None])\n",
    "print('The output of layer4 is a set of neuron activations of shape', tr.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f1b23",
   "metadata": {
    "id": "c08f1b23"
   },
   "source": [
    "How can we make sense of these 512-dimenaional vectors?  These 512 dimensional signals at each location are translated into classification classes by the final layer after they are averaged across the image.  Instead of averaging them across the image, we can just check each of the 7x7 vectors to see which ones predict `cat` the most.  Or we can do the same thing for `dog` (`boxer`).\n",
    "\n",
    "The first step is to get the neuron weights for the cat and the dog neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b7b06",
   "metadata": {
    "id": "af8b7b06"
   },
   "outputs": [],
   "source": [
    "boxer_weights = net.fc.weight[labels.index('boxer')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590c4d7",
   "metadata": {
    "id": "5590c4d7"
   },
   "source": [
    "Each of the weight vectors has 512 dimensions, reflecting all the input weights for each of the neurons.\n",
    "\n",
    "The second step is to dot product (matrix-multply) these weights to each of the 7x7 vectors, each of which is also 512 dimensions.\n",
    "\n",
    "The result will be a 7x7 grid of dot product strengths, which we can render as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16532f7",
   "metadata": {
    "id": "c16532f7"
   },
   "outputs": [],
   "source": [
    "boxer_heatmap = torch.einsum('bcyx, c -> yx', tr.output, boxer_weights)\n",
    "\n",
    "show(show.TIGHT,\n",
    "     [\n",
    "       ['boxer',\n",
    "        rgb_heatmap(boxer_heatmap, mode='nearest')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b682f",
   "metadata": {
    "id": "012b682f"
   },
   "source": [
    "In the following code we smooth the heatmaps and overlay them on top of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0997303",
   "metadata": {
    "id": "a0997303"
   },
   "outputs": [],
   "source": [
    "show(show.TIGHT,\n",
    "    [[['original', im],\n",
    "     ['boxer', overlay(im, rgb_heatmap(boxer_heatmap, im))]\n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ae4a7",
   "metadata": {
    "id": "1f6ae4a7"
   },
   "source": [
    "<b>Exercise 3.6</b>: Repeat the grad-cam to visualise the tiger-cat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30252d9",
   "metadata": {
    "id": "f30252d9"
   },
   "outputs": [],
   "source": [
    "# Type your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119a405",
   "metadata": {
    "id": "7119a405"
   },
   "source": [
    "<b>Exercise 3.6</b>: Now consider the image hungry-cat.jpg\n",
    "\n",
    "Load the image `hungry-cat.jpg` and use grad-cam to visualize the heatmap for the tiger cat and goldfish classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af18f7b",
   "metadata": {
    "id": "1af18f7b"
   },
   "outputs": [],
   "source": [
    "# Type your solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
